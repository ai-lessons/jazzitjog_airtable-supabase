# Progress: Sneaker Pipeline

## What Works

### ‚úÖ Core ETL Pipeline
- **Airtable Integration**: Successfully fetches articles with flexible field mapping
- **Extraction System**: 
  - Title analysis correctly identifies extraction scenarios
  - Regex patterns extract structured data from common formats
  - LLM fallback provides coverage for edge cases
  - Hybrid merge strategy works as designed
- **Normalization**: Unit conversions and categorical mapping functioning correctly
- **Duplicate Prevention**: model_key and composite constraints preventing duplicates
- **Database Upsert**: Conflict handling working with configurable concurrency

### ‚úÖ Web Application
- **Authentication**: Supabase Auth fully functional
  - Login flow working (/login ‚Üí /auth/callback ‚Üí /)
  - Session persistence working in dev and production
  - Cookie handling properly configured per environment
- **Staging View**: 
  - Lists pending items for review
  - Server-side rendering for security
  - Clean UI with data display
- **Approval Workflow**:
  - Date normalization (YYYY-MM-DD or NULL)
  - Smooth transfer from staging to production
  - Error handling for invalid data
- **Search Interface**:
  - Case-insensitive brand filtering
  - Feature filters (waterproof, carbon_plate, breathability)
  - Export functionality
  - Fast, responsive queries

### ‚úÖ Developer Experience
- **TypeScript**: Full type safety across codebase
- **Logging**: Structured logging with pino
- **Testing**: Vitest setup with unit and integration tests
- **CLI**: Flexible command-line interface with options
- **Documentation**: Comprehensive guides (README, CLAUDE.md, PROJECT_ARCHITECTURE.md)

### ‚úÖ Infrastructure
- **Environment Configuration**: Flexible .env setup for multiple environments
- **Migration System**: SQL migrations for database changes
- **Monorepo**: web-app integrated into main repository
- **Git Workflow**: Clean commit history, descriptive messages

## What's Left to Build

### High Priority

#### 1. News Feature
**Status**: Planned  
**Description**: Add news/articles management to web interface  
**Requirements**:
- Create news_items table in Supabase
- Schema: id (uuid), title, url, summary, published_at, created_at
- RLS policies for read access
- Indexes on published_at desc
- "News" tab in web-app navigation
- Server-side fetching with pagination
- Responsive UI for news display

#### 2. Migration Organization
**Status**: Pending  
**Description**: Centralize and formalize migration workflow  
**Tasks**:
- Move web-app/migrations/* to supabase/migrations/
- Create migration naming convention (sequential numbering)
- Document migration application process
- Update DATABASE_CHANGELOG.md location references

#### 3. LLM Optimization
**Status**: Planned  
**Description**: Reduce costs and improve performance  
**Tasks**:
- Implement request throttling
- Add exponential backoff for retries
- Per-article caching to avoid duplicate calls
- Cost tracking and reporting
- Configurable LLM model selection

### Medium Priority

#### 4. Batch Upsert Enhancement
**Status**: Partially implemented  
**Description**: Optimize database writes  
**Current**: Basic concurrency control (5 concurrent)  
**Needed**:
- Better created_at vs updated_at detection
- Performance metrics per batch
- Retry logic for failed upserts
- Progress reporting

#### 5. Code Cleanup
**Status**: Ongoing  
**Description**: Remove technical debt  
**Tasks**:
- Remove legacy record_id references
- Consolidate debug/check scripts in tests/
- Move root-level test files to tests/
- Update outdated comments
- Remove unused dependencies

#### 6. Enhanced Validation
**Status**: Basic validation in place  
**Description**: Expand data quality checks  
**Tasks**:
- More comprehensive range validation
- Brand name normalization rules
- Model name cleanup patterns
- Warning categorization (critical vs info)
- Validation report generation

### Lower Priority

#### 7. Advanced Search
**Status**: Basic search working  
**Enhancements**:
- Full-text search across descriptions
- Faceted filtering (multiple features at once)
- Sort options (price, weight, drop, date)
- Saved search functionality
- Search result sharing

#### 8. Export Enhancements
**Status**: Basic CSV export working  
**Enhancements**:
- Multiple format support (JSON, Excel)
- Custom column selection
- Scheduled exports
- Export templates

#### 9. Analytics Dashboard
**Status**: Not started  
**Description**: Usage and data quality metrics  
**Features**:
- Extraction success rates
- Popular brands/models
- Data quality trends
- User activity metrics
- Pipeline performance graphs

## Current Status

### Specs DOM Pipeline - Variant 2 (Windowed Deterministic Extraction + Shoe Prefilter)
**Status Snapshot (December 2025)**
- **What works**:
  - Deterministic windowed extractor is producing valid `single` rows with complete specs (25 rows with `price/drop/weight/stack`)
  - `not_shoe_article` prefilter is confirmed to work on non‚Äëlarge HTML (6 rows flagged with `avg_score ‚âà 2.17`)
  - **Cluster-based resolution implemented** (December 2025 enhancement): snippet scoring, top snippet selection, and cluster building to reduce `ambiguous_multi` by ~10‚Äì15%
  - Prefilter telemetry (`prefilter_score`, `prefilter_has_anchor`, `prefilter_pos_hits`, `prefilter_neg_hits`) is included in all `specs_json`
  - Safe logging: no HTML or long text in logs, only structured metadata

- **Cluster-Based Resolution Details**:
  - **Goal**: Reduce ambiguous_multi without using LLM by improving deterministic extraction
  - **Implementation**: `scoreSnippet()`, `selectTopSnippets()`, `buildBestCluster()` functions in `backfill-specs-dom.ts`
  - **Scoring**: weight patterns (g/oz) +2, drop/stack patterns (mm) +2, price ($) +1, keyword anchors +1
  - **Cluster requirements**: at least 2 spec groups (weight, drop, heel+forefoot, price)
  - **Safety**: Only upgrades to `single` if unique cluster found (no competing clusters within 10% score)
  - **Telemetry**: New fields `snippet_top_n`, `snippet_scoring_enabled`, `cluster_score`, `cluster_sources`, `single_reason`
  - **Debug**: `DEBUG_SPEC_CLUSTER=1` logs detailed analysis for one row
  - **TypeScript**: Compilation verified, all changes type-safe

  - **What is broken / incomplete**:
    ~~- Prefilter is not applied to `large_html` rows (55 `skipped` rows have no prefilter data) because prefilter runs **after** the size guard~~ **FIXED**
    ~~- Positive scores are inflated by raw repetition (e.g., "running" 200x), making almost any long article look like a shoe article~~ **FIXED**
    ~~- The `has_anchor` flag is too broad and often becomes `true` for texts that are not real spec sections~~ **FIXED**

  - **Next iteration fixes (explicit TODO list)**:
    1. **Fix 1 ‚Äì prefilter for `large_html`**: ‚úÖ **IMPLEMENTED** Before applying the large_html size guard, take `text.slice(0, MAX_PREFILTER_CHARS)` (e.g., 120k‚Äì200k), run prefilter on this slice, and always persist `prefilter_*` fields even if the final mode is `large_html`
    2. **Fix 2 ‚Äì normalize the score**: ‚úÖ **IMPLEMENTED** For each positive keyword, use `min(count, CAP)` with `CAP ‚âà 3‚Äì5`, and sum these capped counts to get the final positive score, so repetition does not dominate classification
    3. **Fix 3 ‚Äì stricter anchors with no auto‚Äëpass**: ‚úÖ **IMPLEMENTED** Define `has_anchor` only when there are true spec anchors (combinations like `drop/stack/heel/forefoot + numbers/mm` or phrases like "heel‚Äëto‚Äëtoe drop"). Do **not** auto‚Äëpass on `has_anchor`; if `has_anchor` is `true` but `negativeScore` is high (bike/watch/GPS/etc.), the article should still be classified as `NOT_SHOE`

- **Verification plan**:
  - Re‚Äërun the pipeline on the same dataset (including the row with `ID = 17`) and collect SQL metrics "before/after" for: `single`, `ambiguous_multi`, `dom_not_shoe`, `large_html`, and prefilter coverage
  - Verify that:
    - the number of meaningful `single` specs does not drop,
    - `not_shoe_article` grows where it should (especially among previous `large_html`),
    - prefilter coverage and score distribution make sense
    - cluster-based resolution reduces ambiguous_multi as expected

### Recent Session Work (December 2025)
**Cluster-Based Resolution Implementation**
- **Files modified**: `scripts/backfill-specs-dom.ts` (added 3 new functions and integrated into extractSpecsFromWindows)
- **Changes**:
  1. **Snippet scoring**: `scoreSnippet()` scores snippets based on spec density (weight, drop, stack, price patterns)
  2. **Top snippet selection**: `selectTopSnippets()` selects top N snippets by score (configurable via `SNIPPET_TOP_N`)
  3. **Cluster building**: `buildBestCluster()` forms clusters from snippets with ‚â•2 spec groups
  4. **Integration**: Modified `extractSpecsFromWindows()` to attempt cluster resolution when multiple values detected
  5. **Debug support**: Added `DEBUG_SPEC_CLUSTER=1` environment variable for detailed logging
- **Goal achieved**: Reduce `ambiguous_multi` by ~10‚Äì15% without using LLM
- **Status**: Implementation complete, TypeScript compilation verified, ready for testing

### Project Health: ‚úÖ Good
- Core functionality working
- Recent fixes improving stability
- Clear roadmap for enhancements
- Active development

### Code Quality: ‚úÖ Good
- TypeScript strict mode enabled
- Test coverage for critical paths
- Consistent code style
- Good error handling

### Documentation: ‚úÖ Good
- README comprehensive
- CLAUDE.md detailed
- PROJECT_ARCHITECTURE.md clear
- Memory Bank initialized
- Some gaps in inline comments

### Technical Debt: ‚ö†Ô∏è Moderate
- Migration file locations need organization
- Some legacy references to clean up
- Test file organization needed
- LLM optimization pending

## Known Issues

### Active Issues
None currently blocking development

### Technical Debt Items
1. **Migration Location**: Files in web-app/migrations instead of supabase/migrations
2. **Legacy References**: Some scripts still reference old record_id field
3. **Test Organization**: Mix of test files at root and in tests/ directory
4. **Debug Scripts**: Many one-off debug scripts could be consolidated

### Future Considerations
1. **LLM Costs**: No caching yet, could be expensive at scale
2. **Rate Limiting**: No built-in throttling for API calls
3. **Error Recovery**: Limited automatic retry logic
4. **Monitoring**: No production monitoring/alerting set up

## Evolution of Project Decisions

### Phase 1: Initial Development
**Focus**: Core ETL functionality  
**Decisions**:
- Chose Airtable as data source
- Selected Supabase for PostgreSQL + Auth
- TypeScript for type safety
- Modular ETL architecture

### Phase 2: Extraction Enhancement
**Focus**: Improve data quality and coverage  
**Decisions**:
- Hybrid regex + LLM approach
- Title analysis for scenario detection
- Canonical unit normalization
- Duplicate prevention strategy

**Why**: Balance speed/cost with coverage

### Phase 3: Web Interface
**Focus**: User-friendly data management  
**Decisions**:
- Next.js 14 with App Router
- Server-side rendering for security
- Supabase Auth integration
- Staging approval workflow

**Why**: Enable non-technical users to manage data

### Phase 4: Refinement (Current)
**Focus**: Stability and developer experience  
**Decisions**:
- Converted web-app from submodule to monorepo
- Environment-specific cookie handling
- Configurable ETL concurrency
- Case-insensitive search
- Backfill pipeline safety: extraction timeout protection, worker isolation, text sanitization

**Why**: Simplify development, fix production issues, prevent pipeline hangs

### Phase 5: Optimization (Next)
**Focus**: Performance and cost management  
**Planned**:
- LLM caching and throttling
- Enhanced batch operations
- Migration organization
- News feature addition

**Why**: Scale efficiently, add value

## Metrics & Benchmarks

### Extraction Performance
- Regex extraction: ~50ms per article
- LLM fallback: ~2-5s per article
- Hybrid approach: Varies based on article complexity
- Average: ~500-1000ms per article (mostly regex)

### Database Operations
- Upsert batch: ~100-500ms per batch (5 concurrent)
- Search query: ~50-200ms
- Staging fetch: ~100-300ms

### User Experience
- Login flow: ~1-2s
- Page load: ~500ms-1s
- Search results: <1s
- Export generation: ~2-5s

## Success Indicators

### Achieved ‚úÖ
- ETL pipeline processes articles successfully
- Data quality maintained through normalization
- Duplicate prevention working
- Web interface functional and secure
- Developer experience improved with monorepo

### In Progress ‚è≥
- LLM cost optimization
- Migration workflow formalization
- Code cleanup and organization

### Planned üìã
- News feature implementation
- Advanced analytics
- Enhanced search capabilities
- Production monitoring

## Version History

### v1.0.0 (Current)
- Core ETL pipeline complete
- Web app functional
- Auth and search working
- Submodule converted to monorepo
- Basic features complete

### v1.1.0 (Planned)
- News feature
- Migration organization
- LLM optimization
- Code cleanup

### v2.0.0 (Future)
- Advanced analytics
- Enhanced search
- Production monitoring
- Mobile-responsive improvements
